{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 飞桨常规赛：视杯视盘分割（GAMMA挑战赛任务三） - 11月第5名方案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 〇、数据解压及项目准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 数据集解压\r\n",
    "! mkdir -p datasets\r\n",
    "! unzip -oq work/Disc_Cup.zip -d datasets\r\n",
    "# 解压测试数据\r\n",
    "! mkdir -p tests\r\n",
    "! unzip -oq work/test.zip -d tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 项目准备\r\n",
    "! git clone --depth=1 https://gitee.com/paddlepaddle/PaddleSeg.git  # paddleseg，github太慢\r\n",
    "! pip install -q patta  # patta\r\n",
    "# ! pip install -q albumentations  # 这个可以在终端通过pip3安装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\r\n",
    "\r\n",
    "sys.path.append('PaddleSeg')  # paddleseg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 一、数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.1 数据信息查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import os\r\n",
    "# import numpy as np\r\n",
    "# from PIL import Image\r\n",
    "\r\n",
    "# 查看大小\r\n",
    "# img_size = []\r\n",
    "# imgs_folder_path = 'datasets/Disc_Cup_Mask'\r\n",
    "# imgs_name = os.listdir(imgs_folder_path)\r\n",
    "# for name in imgs_name:\r\n",
    "#     img_path = os.path.join(imgs_folder_path, name)\r\n",
    "#     img = np.asarray(Image.open(img_path))\r\n",
    "#     img_size.append(img.shape)\r\n",
    "# print(set(img_size))\r\n",
    "# 查看标签数值\r\n",
    "# label = np.asarray(Image.open('datasets/Disc_Cup_Mask/0004.png'))\r\n",
    "# print(set(label.flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```\n",
    "{(1934, 1956), (2000, 2992)}\n",
    "{0, 128, 255}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.2 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import os\r\n",
    "# # import random\r\n",
    "# from PIL import Image\r\n",
    "\r\n",
    "# # 手动选择了10张特征各异的，评估比较有说服力\r\n",
    "# val_name_list = ['0080.jpg', '0070.jpg', '0006.jpg', '0063.jpg', '0086.jpg', \\\r\n",
    "#                  '0075.jpg', '0096.jpg', '0030.jpg', '0062.jpg', '0081.jpg']\r\n",
    "\r\n",
    "# def create_list(data_path):\r\n",
    "#     image_path = os.path.join(data_path, 'Image')\r\n",
    "#     label_path = os.path.join(data_path, 'Disc_Cup_Mask')\r\n",
    "#     data_names = os.listdir(image_path)\r\n",
    "#     # random.shuffle(data_names)  # 打乱数据\r\n",
    "#     with open(os.path.join(data_path, 'train_list.txt'), 'w') as tf:\r\n",
    "#         with open(os.path.join(data_path, 'val_list.txt'), 'w') as vf:\r\n",
    "#             for idx, data_name in enumerate(data_names):\r\n",
    "#                 img = os.path.join('Image', data_name)\r\n",
    "#                 lab = os.path.join('Disc_Cup_Mask', data_name.replace('jpg', 'png'))\r\n",
    "#                 # if idx % 9 == 0:  # 90%的作为训练集\r\n",
    "#                 if data_name in val_name_list:\r\n",
    "#                     vf.write(img + ' ' + lab + '\\n')\r\n",
    "#                 else:\r\n",
    "#                     tf.write(img + ' ' + lab + '\\n')\r\n",
    "#     print('数据列表生成完成')\r\n",
    "\r\n",
    "# data_path = 'datasets'\r\n",
    "# create_list(data_path)  # 生成数据列表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.3 构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddleseg.transforms as T\r\n",
    "from paddleseg.datasets import Dataset\r\n",
    "\r\n",
    "# 构建训练集\r\n",
    "train_transforms = [\r\n",
    "    T.NormSize(),  # 自定义标准化图像大小2000x2000\r\n",
    "    T.LocalEqualHist(),  # 自适应局部直方图均衡化\r\n",
    "    T.RandomHorizontalFlip(),  # 水平翻转\r\n",
    "    T.RandomRotation(im_padding_value=[0, 0, 0]),  # 随机旋转\r\n",
    "    T.Resize(target_size=(1024, 1024)),  # 兼顾大小\r\n",
    "    # T.HighlightClipping(),\r\n",
    "    T.Normalize(\r\n",
    "        [0.3883131, 0.25449154, 0.110598095], \r\n",
    "        [0.26274413, 0.1827712, 0.12587263]),  # 标准化\r\n",
    "]\r\n",
    "train_dataset = Dataset(\r\n",
    "    transforms=train_transforms,\r\n",
    "    dataset_root='datasets',\r\n",
    "    num_classes=3,\r\n",
    "    mode='train',\r\n",
    "    train_path='datasets/train_list.txt',\r\n",
    "    separator=' ',\r\n",
    ")\r\n",
    "# 构建验证集\r\n",
    "val_transforms = [\r\n",
    "    T.NormSize(),\r\n",
    "    T.LocalEqualHist(),\r\n",
    "    T.Resize(target_size=(1024, 1024)),\r\n",
    "    # T.HighlightClipping(),\r\n",
    "    T.Normalize(\r\n",
    "        [0.3883131, 0.25449154, 0.110598095], \r\n",
    "        [0.26274413, 0.1827712, 0.12587263]),    \r\n",
    "]\r\n",
    "val_dataset = Dataset(\r\n",
    "    transforms=val_transforms,\r\n",
    "    dataset_root='datasets',\r\n",
    "    num_classes=3,\r\n",
    "    mode='train',  # 这里用train，不然无法对标签进行NormSize\r\n",
    "    train_path='datasets/val_list.txt',\r\n",
    "    separator=' ',\r\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 测试输出\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "%matplotlib inline\r\n",
    "\r\n",
    "for img, lab in train_dataset:\r\n",
    "    print(set(lab.flatten()))\r\n",
    "    print(img.shape, lab.shape)\r\n",
    "    plt.subplot(121);plt.imshow(img.transpose((1, 2, 0)).astype('uint8'))\r\n",
    "    plt.subplot(122);plt.imshow(lab)\r\n",
    "    plt.show()\r\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.4 查看增强数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ! mkdir -p vis_aug\r\n",
    "# ! mkdir -p vis_aug/img\r\n",
    "# ! mkdir -p vis_aug/lab\r\n",
    "\r\n",
    "# from PIL import Image\r\n",
    "# from tqdm import tqdm\r\n",
    "\r\n",
    "# for idx, (img, lab) in enumerate(tqdm(train_dataset)):\r\n",
    "#     pimg = Image.fromarray(img.transpose(1, 2, 0).astype('uint8'))\r\n",
    "#     plab = Image.fromarray(lab.astype('uint8'))\r\n",
    "#     pimg.save('vis_aug/img/vis_' + str(idx) + '.jpg')\r\n",
    "#     plab.save('vis_aug/lab/vis_' + str(idx) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ! mkdir -p vis_aug_test\r\n",
    "# ! mkdir -p vis_aug_test/img\r\n",
    "# ! mkdir -p vis_aug_test/lab\r\n",
    "\r\n",
    "# from PIL import Image\r\n",
    "# from tqdm import tqdm\r\n",
    "\r\n",
    "# for idx, (img, lab) in enumerate(tqdm(val_dataset)):\r\n",
    "#     pimg = Image.fromarray(img.transpose(1, 2, 0).astype('uint8'))\r\n",
    "#     plab = Image.fromarray(lab.astype('uint8'))\r\n",
    "#     pimg.save('vis_aug_test/img/vis_' + str(idx) + '.jpg')\r\n",
    "#     plab.save('vis_aug_test/lab/vis_' + str(idx) + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.5 均值、方差统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import os\r\n",
    "# import cv2\r\n",
    "# import numpy as np\r\n",
    "# from tqdm import tqdm\r\n",
    " \r\n",
    "# means, stdevs = [], []\r\n",
    "# img_list = []\r\n",
    "# imgs_path = 'vis_aug/img'\r\n",
    "# imgs_name = os.listdir(imgs_path)\r\n",
    "# for name in tqdm(imgs_name):\r\n",
    "#     img = cv2.cvtColor(cv2.imread(os.path.join(imgs_path, name)), cv2.COLOR_BGR2RGB)\r\n",
    "#     img = img[:, :, :, np.newaxis]\r\n",
    "#     img_list.append(img)\r\n",
    "# imgs = np.concatenate(img_list, axis=-1)\r\n",
    "# imgs = imgs.astype(np.float32) / 255.\r\n",
    "# for i in range(3):\r\n",
    "#     pixels = imgs[:, :, i, :].ravel()  # 拉成一行\r\n",
    "#     means.append(np.mean(pixels))\r\n",
    "#     stdevs.append(np.std(pixels))\r\n",
    "# print(means, stdevs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```\n",
    "[0.3883131, 0.25449154, 0.110598095] [0.26274413, 0.1827712, 0.12587263]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 二、训练准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\r\n",
    "import paddle.nn as nn\r\n",
    "from paddleseg.models import OCRNet, HRNet_W18\r\n",
    "\r\n",
    "model = model = OCRNet(\r\n",
    "    backbone=HRNet_W18(),\r\n",
    "    backbone_indices=[0],\r\n",
    "    num_classes=3)\r\n",
    "params = paddle.load('output_ocrnet_hrnet18/best_model/model.pdparams')\r\n",
    "model.set_state_dict(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddleseg.models.losses import CrossEntropyLoss, DiceLoss, MixedLoss\r\n",
    "\r\n",
    "iters = 5000\r\n",
    "batch_size = 4\r\n",
    "base_lr = 3e-4\r\n",
    "\r\n",
    "lr = paddle.optimizer.lr.CosineAnnealingDecay(base_lr, T_max=int(iters // 1.5))\r\n",
    "optimizer = paddle.optimizer.Adam(\r\n",
    "    learning_rate=lr,\r\n",
    "    parameters=model.parameters(),\r\n",
    "    weight_decay=paddle.regularizer.L2Decay(1e-7),\r\n",
    "    grad_clip=paddle.nn.ClipGradByGlobalNorm(clip_norm=1.0))\r\n",
    "losses = {}\r\n",
    "losses['types'] = [MixedLoss([CrossEntropyLoss(), DiceLoss()], [1, 1])] * 5  # 2\r\n",
    "losses['coef'] = [1] * 5  # 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 三、模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddleseg.core import train\r\n",
    "\r\n",
    "train(\r\n",
    "    model=model,\r\n",
    "    train_dataset=train_dataset,\r\n",
    "    val_dataset=val_dataset,\r\n",
    "    optimizer=optimizer,\r\n",
    "    save_dir='output_ocrnet_hrnet18',\r\n",
    "    iters=iters,\r\n",
    "    batch_size=batch_size,\r\n",
    "    save_interval=int(iters/10),\r\n",
    "    log_iters=10,\r\n",
    "    num_workers=0,\r\n",
    "    losses=losses,\r\n",
    "    use_vdl=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 四、模型预测\n",
    "- 使用了水平翻转的tta\n",
    "- 需要将大小和位置统一回原数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:05<00:00,  1.52it/s]\n"
     ]
    }
   ],
   "source": [
    "import paddle\r\n",
    "from paddleseg.models import OCRNet, HRNet_W18\r\n",
    "import paddleseg.transforms as T\r\n",
    "from paddleseg.core import infer\r\n",
    "import os\r\n",
    "from tqdm import tqdm\r\n",
    "from PIL import Image\r\n",
    "import numpy as np\r\n",
    "import patta as tta\r\n",
    "from mytools import tensor2result, restore\r\n",
    "\r\n",
    "def nn_infer(model, imgs_path, is_tta=True):\r\n",
    "    if not os.path.exists('result'):\r\n",
    "        os.mkdir('result')\r\n",
    "    # 预测结果\r\n",
    "    transforms = T.Compose([\r\n",
    "        T.NormSize(),\r\n",
    "        T.LocalEqualHist(),\r\n",
    "        T.Resize(target_size=(1024, 1024)),\r\n",
    "        T.Normalize(\r\n",
    "            [0.3883131, 0.25449154, 0.110598095], \r\n",
    "            [0.26274413, 0.1827712, 0.12587263])\r\n",
    "    ])\r\n",
    "    # 循环预测和保存\r\n",
    "    for img_path in tqdm(imgs_path):\r\n",
    "        H, W = np.asarray(Image.open(img_path)).shape[:2]  # 获取原始的H和W\r\n",
    "        img, _ = transforms(img_path)  # 进行数据预处理\r\n",
    "        img = paddle.to_tensor(img[np.newaxis, :])  # C,H,W -> 1,C,H,W\r\n",
    "        # TTA\r\n",
    "        if is_tta == True:\r\n",
    "            tta_pres = paddle.zeros([1, 3, 1024, 1024])  # 图像大小1024\r\n",
    "            for tta_transform in tta.aliases.hflip_transform ():\r\n",
    "                tta_img = tta_transform.augment_image(img)  # TTA_transforms\r\n",
    "                tta_pre = infer.inference(model, tta_img)  # 预测\r\n",
    "                deaug_pre = tta_transform.deaugment_mask(tta_pre)\r\n",
    "                tta_pres += deaug_pre\r\n",
    "            pre = tta_pres / 2.\r\n",
    "        else:\r\n",
    "            pre = infer.inference(model, img)  # 预测\r\n",
    "        pred = tensor2result(pre)  # 转为颜色对应的array\r\n",
    "        pred = restore(pred, H, W)  # 恢复原始大小\r\n",
    "        pil_img = Image.fromarray(pred)\r\n",
    "        pil_img.save(os.path.join('result', img_path.split('/')[-1].replace('jpg', 'bmp')), 'bmp')\r\n",
    "\r\n",
    "# 网络准备\r\n",
    "model_path = 'output_ocrnet_hrnet18/best_model/model.pdparams'\r\n",
    "model = OCRNet(\r\n",
    "    backbone=HRNet_W18(),\r\n",
    "    backbone_indices=[0],\r\n",
    "    num_classes=3)\r\n",
    "params = paddle.load(model_path)\r\n",
    "model.set_state_dict(params)\r\n",
    "model.eval()\r\n",
    "# 预测文件\r\n",
    "# set_path = 'datasets'\r\n",
    "# list_file = 'datasets/val_list.txt'\r\n",
    "# imgs_path = []\r\n",
    "# with open(list_file, 'r') as f:\r\n",
    "#     datas_path = f.readlines()\r\n",
    "#     for data_path in datas_path:\r\n",
    "#         imgs_path.append(os.path.join(set_path, data_path.split(' ')[0].strip()))\r\n",
    "test_folder = \"tests\"\r\n",
    "imgs_path = os.listdir(test_folder)\r\n",
    "imgs_path = [os.path.join(test_folder, name) for name in imgs_path]\r\n",
    "# 预测\r\n",
    "nn_infer(model, imgs_path, is_tta=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 五、预测后处理\n",
    "1. 闭运算填充孔洞\n",
    "2. 保留最大联通区，去掉其他小的联通区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:18<00:00,  5.54it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\r\n",
    "import numpy as np\r\n",
    "from tqdm import tqdm\r\n",
    "from PIL import Image\r\n",
    "from aftercure import one_package_service\r\n",
    "\r\n",
    "pred_folder = 'result'\r\n",
    "save_folder = 'af_result'\r\n",
    "if not os.path.exists(save_folder):\r\n",
    "    os.mkdir(save_folder)\r\n",
    "imgs_name = os.listdir(pred_folder)\r\n",
    "for name in tqdm(imgs_name):\r\n",
    "    img_path = os.path.join(pred_folder, name)\r\n",
    "    img = np.asarray(Image.open(img_path))\r\n",
    "    result = Image.fromarray(one_package_service(img, k_size=300))\r\n",
    "    result.save(img_path.replace(pred_folder, save_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 六、结果评价\n",
    "Dice系数作为分割结果测评的评价指标：\n",
    "\n",
    "$\\text { Dice }=\\frac{2|X \\cap Y|}{|X|+|Y|}$\n",
    "\n",
    "其中，X代表金标准中分割目标像素点集合，Y代表预测结果中分割目标像素点集合，公式中 |X∩Y| 是X和Y之间的交集，|X|和|Y|分表表示X和Y的元素的个数。\n",
    "此外，我们使用平均绝对误差（Mean Absolute Error）来测量样本视杯视盘分割结果与金标准之间的垂直杯盘比差异。垂直杯盘比有直接的临床相关性，可辅助评估青光眼的进展。求解方式为计算垂直方向上视杯区域和视盘区域最大直径的比值。对于任务三，最终的评分综合了视杯分割结果Dice系数、视盘分割结果Dice系数，和垂直杯盘比数值的MAE：\n",
    "\n",
    "$\\text { Score }_{\\text {task } 3}=0.25 * \\text { Dice }_{\\text {cup }}+0.35 * \\text { Dice }_{d i s c}+0.04 * \\frac{1}{M A E+0.1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from mytools import get_score\r\n",
    "\r\n",
    "# score = get_score('af_result', 'datasets/Disc_Cup_Mask', False)\r\n",
    "# print('当前得分为:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```\n",
    "当前得分为: 0.7742816652233033\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
